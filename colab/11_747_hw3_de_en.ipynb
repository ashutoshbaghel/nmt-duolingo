{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11-747-hw3-bpe-de-en.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDdyw8ZxZbjB",
        "colab_type": "text"
      },
      "source": [
        "# Get P100 GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwQcaX0-ZjQR",
        "colab_type": "code",
        "outputId": "2bb2dc3b-7a06-471b-9369-eb00b93885d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Apr  3 11:02:25 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   63C    P0    42W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzEjlywnjwZL",
        "colab_type": "text"
      },
      "source": [
        "# Get the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYc7LTiBjJP0",
        "colab_type": "text"
      },
      "source": [
        "## Load all the required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EE6Na4n6jIRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchtext\n",
        "from torchtext.data import Field, BucketIterator, Iterator\n",
        "from torchtext.datasets import TranslationDataset\n",
        "from collections import defaultdict\n",
        "from allennlp.training.learning_rate_schedulers.noam import NoamLR\n",
        "\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "import os\n",
        "import torch\n",
        "import copy\n",
        "import time\n",
        "import math\n",
        "\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F\n",
        "import dill as pickle\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AIsIG25j_kS",
        "colab_type": "text"
      },
      "source": [
        "## Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfCn3xSsjv0w",
        "colab_type": "code",
        "outputId": "43f003c2-cc21-4466-f26c-f8acc64fece9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# link to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWuEVpp7LyWA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpePmf5lkCN7",
        "colab_type": "text"
      },
      "source": [
        "## Read data into memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47Y-GLLFsWI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dir_path = '/content/drive/My Drive/Spring-20/11-747/11-747 Project/de-en/data/fairseq_data'\n",
        "src = Field(fix_length=100)\n",
        "trg = Field(init_token = \"<sos>\", eos_token = \"<eos>\", fix_length=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LpQ4jFussQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_loader(mode):\n",
        "    data = TranslationDataset(\n",
        "        path=dir_path + f\"/{mode}\", exts=('.de', '.en'),\n",
        "        fields=(src, trg))\n",
        "\n",
        "    iterator = None\n",
        "    if mode == \"train\" or mode == \"dev\":\n",
        "        if mode == \"train\":\n",
        "          src.build_vocab(data)\n",
        "          trg.build_vocab(data)\n",
        "\n",
        "        iterator = BucketIterator(dataset=data, batch_size=128,\n",
        "            sort_key=lambda x: data.interleave_keys(len(x.src), len(x.trg)))\n",
        "    \n",
        "    else:\n",
        "        iterator = Iterator(dataset=data, batch_size=64, train=False, \n",
        "                            shuffle=False, sort=False)\n",
        "    \n",
        "    return iterator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCwTznJvNgnl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train dataloader\n",
        "train_iter = data_loader(\"train\")\n",
        "\n",
        "# Validation dataloader\n",
        "val_iter = data_loader(\"dev\")\n",
        "\n",
        "# Test dataloader\n",
        "test_iter = data_loader(\"test\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvaAENmB7W_Q",
        "colab_type": "text"
      },
      "source": [
        "## Model Parameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ltr_WsW233M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Configuration(object):\n",
        "  def __init__(self, source, target):\n",
        "    self.src_data = source\n",
        "    self.trg_data = target\n",
        "    self.src_lang = 'de'\n",
        "    self.trg_lang = 'en'\n",
        "    self.src_pad = src.vocab.stoi['<pad>']\n",
        "    self.trg_pad = trg.vocab.stoi['<pad>']\n",
        "    self.epochs = 50\n",
        "    self.n_layers = 6\n",
        "    self.heads = 8\n",
        "    self.dropout = 0.1\n",
        "    self.printevery = 100\n",
        "    self.lr = 5e-4\n",
        "    self.emb_dim = 512\n",
        "    self.ff_hsize = 1024\n",
        "    self.max_strlen = 100\n",
        "    self.checkpoint = 0\n",
        "    self.device = 0\n",
        "    self.clip_norm = 0.0\n",
        "    self.k = 5\n",
        "    self.max_len = 100\n",
        "    self.factor=2\n",
        "    self.warmup=4000\n",
        "    self.length_penalty = 0.7\n",
        "\n",
        "\n",
        "opt = Configuration(src, trg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2F3LGZBx3BT",
        "colab_type": "text"
      },
      "source": [
        "# My Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxJ-6FkH6Zsa",
        "colab_type": "text"
      },
      "source": [
        "## Embedder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGOs93126bEK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Embedder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "    def forward(self, x):\n",
        "        return self.embed(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E-bveci6bMB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, opt, max_seq_len=100):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=opt.dropout)\n",
        "        self.dim = opt.emb_dim\n",
        "        pe = torch.zeros(max_seq_len, self.dim)\n",
        "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        # 1000 ^ (2i / dmodel) = e ^ (2i) * -log(1000)\n",
        "        div_term = torch.exp(torch.arange(0, self.dim, 2).float() \\\n",
        "                            * (-math.log(10000.0) / self.dim))\n",
        "        \n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        \n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        " \n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x * math.sqrt(self.dim)\n",
        "        pe = Variable(self.pe[:,:x.size(1)], requires_grad=False)\n",
        "        pe = pe.to(device)\n",
        "        x = x + pe\n",
        "        return self.dropout(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wA-6VFibyTUL",
        "colab_type": "text"
      },
      "source": [
        "## Sublayers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FBY_eA0tJ1UJ",
        "colab": {}
      },
      "source": [
        "class Norm(nn.Module):\n",
        "    def __init__(self, d_model, eps=1e-5):\n",
        "        super().__init__()\n",
        "    \n",
        "        self.size = d_model\n",
        "        \n",
        "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
        "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
        "        \n",
        "        self.eps = eps\n",
        "    \n",
        "    def forward(self, x):\n",
        "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
        "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
        "        return norm\n",
        "\n",
        "def attention(q, k, v, d_k, mask, dropout): \n",
        "    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)   \n",
        "    scores = dropout(F.softmax(scores.masked_fill(mask == 0, -1e9), dim=-1))\n",
        "        \n",
        "    output = torch.matmul(scores, v)\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqfRMYV1WKkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, opt):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.d_model = opt.emb_dim\n",
        "        self.d_k = int(self.d_model/ opt.heads)\n",
        "        self.h = opt.heads\n",
        "        \n",
        "        self.q_linear = nn.Linear(self.d_model, self.d_model)\n",
        "        self.v_linear = nn.Linear(self.d_model, self.d_model)\n",
        "        self.k_linear = nn.Linear(self.d_model, self.d_model)\n",
        "        \n",
        "        self.dropout = nn.Dropout(opt.dropout)\n",
        "        self.out = nn.Linear(self.d_model, self.d_model)\n",
        "    \n",
        "    def forward(self, q, k, v, mask):\n",
        "        k = self.k_linear(k).view(q.size(0), -1, self.h, self.d_k).transpose(1,2)\n",
        "        q = self.q_linear(q).view(q.size(0), -1, self.h, self.d_k).transpose(1,2)\n",
        "        v = self.v_linear(v).view(q.size(0), -1, self.h, self.d_k).transpose(1,2)\n",
        "\n",
        "        mask = mask.unsqueeze(1)\n",
        "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
        "        concat = scores.transpose(1,2).contiguous() \\\n",
        "                .view(q.size(0), -1, self.d_model)\n",
        "        output = self.out(concat)\n",
        "    \n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sUHGRT1yhfO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, opt):\n",
        "        super().__init__() \n",
        "\n",
        "        linear_1 = nn.Linear(opt.emb_dim, opt.ff_hsize)\n",
        "        dropout = nn.Dropout(opt.dropout)\n",
        "        linear_2 = nn.Linear(opt.ff_hsize, opt.emb_dim)\n",
        "\n",
        "        self.layers = nn.Sequential(linear_1, nn.ReLU(), dropout, linear_2)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        self.layers(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uTRXKnmyJ0e",
        "colab_type": "text"
      },
      "source": [
        "## Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faUPPkKox6b2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, opt):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(opt.emb_dim)\n",
        "        self.norm_2 = Norm(opt.emb_dim)\n",
        "\n",
        "        self.dropout_1 = nn.Dropout(opt.dropout)\n",
        "        self.dropout_2 = nn.Dropout(opt.dropout)\n",
        "        \n",
        "        self.attn = MultiHeadAttention(opt)\n",
        "        \n",
        "        self.ff = FeedForward(opt)\n",
        "\n",
        "        self.d = math.sqrt(opt.emb_dim // opt.heads)\n",
        "        \n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        '''\n",
        "        This implementation follows the Tensor2Tensor implementation\n",
        "        instead of the original paper \"Attention is all you need\"\n",
        "        The Norm is applied to the input first, then self attention\n",
        "        is applied to the sub-layer.\n",
        "        '''\n",
        "\n",
        "        x = self.norm_1(x)\n",
        "        x1 = x + self.dropout_1(self.attn(x, x, x, mask))\n",
        "\n",
        "        x1 = self.norm_2(x1)\n",
        "        x2 = x1 + self.dropout_2(self.ff(x1))\n",
        "\n",
        "        return x2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Luo197iSyAIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, opt):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(opt.emb_dim)\n",
        "        self.norm_2 = Norm(opt.emb_dim)\n",
        "        self.norm_3 = Norm(opt.emb_dim)\n",
        "        \n",
        "        self.dropout_1 = nn.Dropout(opt.dropout)\n",
        "        self.dropout_2 = nn.Dropout(opt.dropout)\n",
        "        self.dropout_3 = nn.Dropout(opt.dropout)\n",
        "\n",
        "        self.attn_1 = MultiHeadAttention(opt)\n",
        "        \n",
        "        self.attn_2 = MultiHeadAttention(opt)\n",
        "        \n",
        "        self.ff = FeedForward(opt)\n",
        "\n",
        "        self.d = math.sqrt(opt.emb_dim // opt.heads)\n",
        "\n",
        "    def forward(self, x, e_outputs, src_mask, trg_mask):\n",
        "        '''\n",
        "        This implementation follows the Tensor2Tensor implementation\n",
        "        instead of the original paper \"Attention is all you need\"\n",
        "        The Norm is applied to the input first, then self attention\n",
        "        is applied to the sub-layer.\n",
        "        '''\n",
        "        x = self.norm_1(x)\n",
        "        x1 = x + self.dropout_1(self.attn_1(x, x, x, trg_mask))\n",
        "\n",
        "        x1 = self.norm_2(x1)\n",
        "        x2 = x1 + self.dropout_2(self.attn_2(x1, \n",
        "                                             e_outputs, \n",
        "                                             e_outputs,\n",
        "                                             src_mask))\n",
        "\n",
        "        x2 = self.norm_3(x2)\n",
        "        x3 = x2 + self.dropout_3(self.ff(x2))\n",
        "\n",
        "        return x3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Egy_iM_LX-xZ",
        "colab_type": "text"
      },
      "source": [
        "## Masks\n",
        "\n",
        "There are two type of masks,\n",
        "\n",
        "1.   To mask out the padded input sequence to prevent attention score being applied to them.\n",
        "2.   To mask out the future time steps while decoding\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhEacPK98UmB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nopeak_mask(size, opt):\n",
        "    np_mask = torch.triu(torch.ones((1, size, size)), diagonal=1)\n",
        "    np_mask =  Variable(np_mask == 0)\n",
        "    return np_mask\n",
        "\n",
        "def create_masks(src, trg, opt):\n",
        "    \n",
        "    src_mask = (src != opt.src_pad).unsqueeze(-2)\n",
        "\n",
        "    if trg is not None:\n",
        "        trg_mask = (trg != opt.trg_pad).unsqueeze(-2)\n",
        "        size = trg.size(1)\n",
        "        np_mask = nopeak_mask(size, opt)\n",
        "        np_mask = np_mask.to(device)\n",
        "        trg_mask = trg_mask & np_mask\n",
        "    else:\n",
        "        trg_mask = None\n",
        "        \n",
        "    return src_mask, trg_mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1JeLvspxsn_",
        "colab_type": "text"
      },
      "source": [
        "## Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHLRvCPvxt9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, opt):\n",
        "        super().__init__()\n",
        "        self.N = opt.n_layers\n",
        "        self.embed = Embedder(vocab_size, opt.emb_dim)\n",
        "        self.pe = PositionalEncoder(opt)\n",
        "        self.layers = get_clones(EncoderLayer(opt), self.N)\n",
        "        self.norm = Norm(opt.emb_dim)\n",
        "\n",
        "    def forward(self, src, mask):\n",
        "        x = self.embed(src)\n",
        "        x = self.pe(x)\n",
        "\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, mask)\n",
        "\n",
        "        return self.norm(x)\n",
        "    \n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, opt):\n",
        "        super().__init__()\n",
        "        self.N = opt.n_layers\n",
        "        self.embed = Embedder(vocab_size, opt.emb_dim)\n",
        "        self.pe = PositionalEncoder(opt)\n",
        "        self.layers = get_clones(DecoderLayer(opt), self.N)\n",
        "        self.norm = Norm(opt.emb_dim)\n",
        "\n",
        "    def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
        "        x = self.embed(trg)\n",
        "        x = self.pe(x)\n",
        "\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
        "\n",
        "        return self.norm(x)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, s_len, t_len, opt):\n",
        "        super().__init__()\n",
        "        assert opt.emb_dim % opt.heads == 0\n",
        "        assert opt.dropout < 1\n",
        "\n",
        "        self.opt = opt\n",
        "\n",
        "        self.encoder = Encoder(s_len, self.opt)\n",
        "        self.decoder = Decoder(t_len, self.opt)\n",
        "        self.out = nn.Linear(opt.emb_dim, t_len)\n",
        "\n",
        "    def forward(self, src_seq, trg_seq, src_mask, trg_mask):\n",
        "        e_outputs = self.encoder(src_seq, src_mask)\n",
        "        d_output = self.decoder(trg_seq, e_outputs, src_mask, trg_mask)\n",
        "        output = self.out(d_output)\n",
        "        return output\n",
        "\n",
        "    def decode(self, decoder_input, encoder_output, src_mask, trg_mask):\n",
        "        return self.out(model.decoder(decoder_input,\n",
        "                                      encoder_output, \n",
        "                                      src_mask, \n",
        "                                      trg_mask))\n",
        "\n",
        "def init_model_params(model):    \n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65l9LqC1vYv7",
        "colab_type": "text"
      },
      "source": [
        "# Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGJrw0s0vY7e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_std_opt(model, optimizer):\n",
        "    return NoamLR(optimizer,\n",
        "                  opt.emb_dim,\n",
        "                  opt.warmup,\n",
        "                  opt.factor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkfLyjnUfX5-",
        "colab_type": "text"
      },
      "source": [
        "## Decoder and Beam Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwzVp2KTfd8s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BeamSearch():\n",
        "\n",
        "    def __init__(self, src, trg, model, opt):\n",
        "        self.src = src\n",
        "        self.trg = trg\n",
        "        self.model = model\n",
        "        self.opt = opt\n",
        "        self.init_tok = trg.vocab.stoi['<sos>']\n",
        "\n",
        "\n",
        "    def beam_search(self, encoded_seq, src_mask):\n",
        "        assert encoded_seq.shape[0] == opt.k\n",
        "        assert encoded_seq.shape[1] == opt.max_len\n",
        "\n",
        "        eos_tok = self.trg.vocab.stoi['<eos>']\n",
        "        outputs = torch.LongTensor([[self.init_tok]]).to(device)\n",
        "        ind = None\n",
        "\n",
        "        for i in range(1, self.opt.max_len):\n",
        "            trg_mask = nopeak_mask(i, self.opt).to(device)\n",
        "\n",
        "            if i == 1:\n",
        "                out = self.model.decode(outputs, \n",
        "                                        encoded_seq[0], \n",
        "                                        src_mask, \n",
        "                                        trg_mask)\n",
        "            else:\n",
        "                out = self.model.decode(outputs[:, :i], \n",
        "                                        encoded_seq, \n",
        "                                        src_mask, \n",
        "                                        trg_mask)\n",
        "            out = F.softmax(out, dim=-1)\n",
        "\n",
        "            probs, ix = out[:, -1].data.topk(self.opt.k)\n",
        "\n",
        "            if i == 1:\n",
        "                log_scores = torch.Tensor([math.log(prob) \\\n",
        "                                           for prob in probs.data[0]]) \\\n",
        "                                           .unsqueeze(0)\n",
        "                outputs = torch.zeros(opt.k, opt.max_len).long().to(device)\n",
        "\n",
        "                outputs[:, 0] = self.init_tok\n",
        "                outputs[:, 1] = ix[0]\n",
        "\n",
        "                continue\n",
        "\n",
        "            log_probs = torch.Tensor([math.log(p) \\\n",
        "                                        for p in probs.data.view(-1)]) \\\n",
        "                                    .view(opt.k, -1) \\\n",
        "                                    + log_scores.transpose(0,1)\n",
        "\n",
        "            k_probs, k_ix = log_probs.view(-1).topk(opt.k)\n",
        "\n",
        "            outputs[:, :i] = outputs[k_ix // opt.k, :i]\n",
        "            outputs[:, i] = ix[k_ix // opt.k, k_ix % opt.k]\n",
        "\n",
        "            log_scores = k_probs.unsqueeze(0)\n",
        "\n",
        "            sentence_lengths = torch.zeros(len(outputs), \n",
        "                                            dtype=torch.long).to(device)\n",
        "\n",
        "            for vec in (outputs==eos_tok).nonzero():\n",
        "                i = vec[0]\n",
        "                if sentence_lengths[i]==0:\n",
        "                    sentence_lengths[i] = vec[1]\n",
        "\n",
        "            num_finished_sentences = len([s for s in sentence_lengths \\\n",
        "                                            if s > 0])\n",
        "            \n",
        "            if num_finished_sentences == opt.k:\n",
        "                alpha = opt.length_penalty\n",
        "                div = 1/(sentence_lengths.type_as(log_scores)**alpha)\n",
        "                _, ind = torch.max(log_scores * div, 1)\n",
        "                ind = ind.data[0]\n",
        "                break\n",
        "        \n",
        "        if ind is None:\n",
        "            length = (outputs[0]==eos_tok).nonzero()   \n",
        "            return [self.trg.vocab.itos[tok] for tok in outputs[0]]\n",
        "        else:\n",
        "            length = (outputs[ind]==eos_tok).nonzero()[0]\n",
        "            return [self.trg.vocab.itos[tok] for tok in outputs[ind][1:length]]\n",
        "\n",
        "    def encode_batch(self, test_batch):\n",
        "        src_mask = (test_batch != self.src.vocab.stoi['<pad>']).unsqueeze(-2)\n",
        "        encoded_batch = self.model.encoder(test_batch, src_mask)\n",
        "        return encoded_batch, src_mask.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfVZKQyRfUyh",
        "colab_type": "text"
      },
      "source": [
        "# Generate Translation and Evaluate Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5Xb4HZyWvsG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate_sentence(test_batch, b_search, opt): \n",
        "    final_output = []\n",
        "    encoded_batch, src_masks = b_search.encode_batch(test_batch)\n",
        "\n",
        "    for i, (encoded_seq, src_mask) in enumerate(zip(encoded_batch, src_masks)):\n",
        "        encoded_seqs =  torch.zeros(opt.k, \n",
        "                                    encoded_seq.size(-2), \n",
        "                                    encoded_seq.size(-1))\n",
        "        encoded_seqs[:, :] = encoded_seq[0]\n",
        "        encoded_seqs = encoded_seqs.to(device)\n",
        "\n",
        "        sentence = b_search.beam_search(encoded_seqs, src_mask)\n",
        "        final_output.append(sentence)\n",
        "\n",
        "        del encoded_seqs\n",
        "    \n",
        "    del encoded_batch\n",
        "    del src_masks\n",
        "\n",
        "    return final_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXrV2ZdH0zJk",
        "colab_type": "text"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1lYoV8yYtH3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Translator():\n",
        "    def __init__(self, model, opt):\n",
        "        self.opt = opt\n",
        "        self.model = model\n",
        "\n",
        "    def train(self, check_path):\n",
        "        print(\"training model...\")\n",
        "        opt = self.opt\n",
        "        model = self.model\n",
        "\n",
        "        model.train()\n",
        "        start = time.time()\n",
        "\n",
        "        best_acc = []\n",
        "        step = 0\n",
        "\n",
        "        for epoch in range(opt.epochs):  \n",
        "            total_loss = 0\n",
        "\n",
        "            for i, batch in enumerate(train_iter):\n",
        "                src_seq = batch.src.transpose(0,1)\n",
        "                trg_seq = batch.trg.transpose(0,1)\n",
        "\n",
        "                src_seq, trg_seq = src_seq.to(device), trg_seq.to(device)\n",
        "                trg_input = trg_seq[:, :-1].to(device)\n",
        "                \n",
        "                src_mask, trg_mask = create_masks(src_seq, trg_input, opt)\n",
        "                \n",
        "                src_mask = src_mask.to(device)\n",
        "                trg_mask = trg_mask.to(device)\n",
        "\n",
        "                preds = model(src_seq, trg_input, src_mask, trg_mask)\n",
        "\n",
        "                ys = trg_seq[:, 1:].contiguous().view(-1)\n",
        "                opt.optimizer.zero_grad()\n",
        "\n",
        "                loss = F.cross_entropy(preds.view(-1, preds.size(-1)), \n",
        "                                       ys, \n",
        "                                       ignore_index=opt.trg_pad)\n",
        "                loss.backward()\n",
        "\n",
        "                opt.optimizer.step()\n",
        "                opt.scheduler.step_batch(step)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                step +=1\n",
        "                \n",
        "                if (i + 1) % opt.printevery == 0:\n",
        "                    p = int(100 * (i + 1) / len(train_iter))\n",
        "                    print(f\"total loss: {total_loss}\")\n",
        "                    avg_loss = total_loss/opt.printevery\n",
        "                    elapsed_time = int((time.time() - start)//60)\n",
        "                    print(f\"{elapsed_time}m | epoch {epoch} | \",\n",
        "                          f\"{p}% | loss = {avg_loss}\")\n",
        "                    total_loss = 0\n",
        "\n",
        "                del src_seq\n",
        "                del trg_seq\n",
        "                del src_mask\n",
        "                del trg_mask\n",
        "    \n",
        "            val_loss = self.eval(model)\n",
        "            self.best_checkpoints(best_acc, val_loss, epoch, check_path)\n",
        "            self.save_checkpoint(epoch, model, opt, check_path)\n",
        "\n",
        "            print(f\"{(time.time() - start)//60}m:  | epoch {epoch + 1}  100% \\\n",
        "                    loss = {avg_loss:.3f}\\nepoch {epoch + 1} | complete, \\\n",
        "                    loss = {avg_loss:.3f}, val_loss = {val_loss:0.3f}\")\n",
        "            \n",
        "    def eval(self, model):\n",
        "        model = self.model\n",
        "        model.eval()\n",
        "        total_loss = 0\n",
        "\n",
        "        for i, batch in enumerate(val_iter):\n",
        "            with torch.no_grad():\n",
        "\n",
        "                src = batch.src.transpose(0,1)\n",
        "                trg = batch.trg.transpose(0,1)\n",
        "                src, trg = src.to(device), trg.to(device)\n",
        "                trg_input = trg[:, :-1]\n",
        "\n",
        "                src_mask, trg_mask = create_masks(src, trg_input, opt)\n",
        "                \n",
        "                src_mask = src_mask.to(device)\n",
        "                trg_mask = trg_mask.to(device)\n",
        "\n",
        "                preds = model(src, trg_input, src_mask, trg_mask)\n",
        "                ys = trg[:, 1:].contiguous().view(-1)\n",
        "\n",
        "                loss = F.cross_entropy(preds.view(-1, preds.size(-1)), \n",
        "                                    ys, ignore_index=opt.trg_pad)\n",
        "                \n",
        "                total_loss += loss.item()\n",
        "\n",
        "                del src\n",
        "                del trg\n",
        "                del src_mask\n",
        "                del trg_mask\n",
        "\n",
        "        val_loss = total_loss / len(val_iter)\n",
        "        model.train()\n",
        "        return val_loss\n",
        "\n",
        "    def test(self):\n",
        "        print(\"testing model...\")\n",
        "        self.model.eval() \n",
        "        b_search = BeamSearch(src, trg, self.model, self.opt)\n",
        "                \n",
        "        for i, batch in enumerate(test_iter):\n",
        "            test_batch = batch.src.transpose(0,1).to(device)\n",
        "            output = translate_sentence(test_batch, b_search, opt)\n",
        "            append_to_list(output)\n",
        "\n",
        "    def best_checkpoints(self, best_acc, val_loss, epoch, check_path):\n",
        "        best_acc.append((epoch, val_loss))\n",
        "\n",
        "        with open(os.path.join(check_path, \"stats.txt\"), \"a+\") as fl:\n",
        "            size = 5 if len(best_acc) > 4 else len(best_acc)\n",
        "            best = sorted(best_acc, key=s_key)[:size]\n",
        "            indices = [str(a[0]) for a in best]\n",
        "            pt = \" \".join(indices)\n",
        "            fl.write(f\"epoch_num: {epoch}, val_loss: {val_loss}, \\\n",
        "                    top 5 checkpoints: {pt}\\n\")\n",
        "            fl.write(\"====\\n\")\n",
        "            fl.close()\n",
        "\n",
        "        return\n",
        "\n",
        "    def make_checkpoint_dir(self, path):\n",
        "        d = datetime.now()\n",
        "        EST = pytz.timezone('US/Eastern')\n",
        "        d = d.astimezone(EST)\n",
        "        fd = str(d.strftime(\"afternorm-%d-%H_%M_%S\"))\n",
        "\n",
        "        check_path = os.path.join(path, fd)\n",
        "\n",
        "        try:\n",
        "            os.mkdir(check_path)\n",
        "        except OSError:\n",
        "            print(\"Creation of the directory %s failed\" % check_path)\n",
        "        else:\n",
        "            print(\"Successfully created the directory %s \" % check_path)\n",
        "        return check_path\n",
        "\n",
        "    \n",
        "    def save_checkpoint(self, epoch, model, opt, check_path):\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': opt.optimizer.state_dict(),\n",
        "            }, \n",
        "            os.path.join(check_path, \n",
        "                         'transformer_'  + str(epoch) + '_model.pth'))\n",
        "        \n",
        "    def load_checkpoint(self, checkpoint_path, model):\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        opt.optimizer.load_state_dict(\n",
        "            checkpoint['optimizer_state_dict'])\n",
        "        model = model.to(self.opt.device)\n",
        "        self.model = model\n",
        "        \n",
        "def s_key(lst):\n",
        "    return lst[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaxmL5MH7f_W",
        "colab_type": "text"
      },
      "source": [
        "## Declare the Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kz206xOdHVf",
        "colab_type": "code",
        "outputId": "ebe031dc-3f4b-402f-94f8-a7e2d5a19073",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "mode = \"train\"\n",
        "saved_checkpoint_path = \"/content/drive/My Drive/Spring-20/11-747/11-747 Project/checkpoints/afternorm-01-02_25_12/transformer_5_model.pth\"\n",
        "check_dir = '/content/drive/My Drive/Spring-20/11-747/11-747 Project/checkpoints'\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    transformer = Transformer(len(src.vocab), len(trg.vocab), opt)\n",
        "    model = init_model_params(transformer)\n",
        "    model = model.to(device)\n",
        "\n",
        "    opt.optimizer = torch.optim.Adam(model.parameters(), \n",
        "                                     lr=opt.lr, \n",
        "                                     betas=(0.9, 0.98), \n",
        "                                     weight_decay=0.0001, \n",
        "                                     eps=1e-9)\n",
        "    \n",
        "    opt.scheduler = get_std_opt(model, opt.optimizer)\n",
        "\n",
        "    translator = Translator(model, opt)\n",
        "\n",
        "    if mode == \"train\":\n",
        "        check_path = translator.make_checkpoint_dir(check_dir)\n",
        "        translator.train(check_path)\n",
        "    elif mode == \"test\":\n",
        "        translator.load_checkpoint(saved_checkpoint_path, model)\n",
        "        translator.test()\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully created the directory /content/drive/My Drive/Spring-20/11-747/11-747 Project/checkpoints/afternorm-03-07_02_59 \n",
            "training model...\n",
            "total loss: 784.813802242279\n",
            "0m | epoch 0 |  7% | loss = 7.84813802242279\n",
            "total loss: 673.3436665534973\n",
            "1m | epoch 0 |  15% | loss = 6.733436665534973\n",
            "total loss: 623.1265358924866\n",
            "2m | epoch 0 |  23% | loss = 6.231265358924865\n",
            "total loss: 607.5639386177063\n",
            "3m | epoch 0 |  31% | loss = 6.075639386177063\n",
            "total loss: 576.9185843467712\n",
            "4m | epoch 0 |  39% | loss = 5.769185843467713\n",
            "total loss: 545.6001148223877\n",
            "5m | epoch 0 |  47% | loss = 5.456001148223877\n",
            "total loss: 523.3435544967651\n",
            "6m | epoch 0 |  55% | loss = 5.2334355449676515\n",
            "total loss: 504.4624056816101\n",
            "7m | epoch 0 |  63% | loss = 5.044624056816101\n",
            "total loss: 492.979220867157\n",
            "8m | epoch 0 |  71% | loss = 4.92979220867157\n",
            "total loss: 480.15998554229736\n",
            "9m | epoch 0 |  79% | loss = 4.801599855422974\n",
            "total loss: 469.4180254936218\n",
            "10m | epoch 0 |  87% | loss = 4.694180254936218\n",
            "total loss: 458.7497625350952\n",
            "11m | epoch 0 |  95% | loss = 4.587497625350952\n",
            "11.0m:  | epoch 1  100%                     loss = 4.587\n",
            "epoch 1 | complete,                     loss = 4.587, val_loss = 4.391\n",
            "total loss: 438.5997967720032\n",
            "12m | epoch 1 |  7% | loss = 4.385997967720032\n",
            "total loss: 429.8842988014221\n",
            "13m | epoch 1 |  15% | loss = 4.298842988014221\n",
            "total loss: 422.34843349456787\n",
            "14m | epoch 1 |  23% | loss = 4.223484334945678\n",
            "total loss: 416.173544883728\n",
            "15m | epoch 1 |  31% | loss = 4.16173544883728\n",
            "total loss: 410.07572531700134\n",
            "16m | epoch 1 |  39% | loss = 4.100757253170014\n",
            "total loss: 405.5812556743622\n",
            "17m | epoch 1 |  47% | loss = 4.055812556743621\n",
            "total loss: 402.1333215236664\n",
            "18m | epoch 1 |  55% | loss = 4.021333215236663\n",
            "total loss: 398.03005814552307\n",
            "19m | epoch 1 |  63% | loss = 3.980300581455231\n",
            "total loss: 395.55870747566223\n",
            "20m | epoch 1 |  71% | loss = 3.955587074756622\n",
            "total loss: 392.2933020591736\n",
            "21m | epoch 1 |  79% | loss = 3.922933020591736\n",
            "total loss: 385.09384179115295\n",
            "21m | epoch 1 |  87% | loss = 3.8509384179115296\n",
            "total loss: 369.98654985427856\n",
            "22m | epoch 1 |  95% | loss = 3.6998654985427857\n",
            "23.0m:  | epoch 2  100%                     loss = 3.700\n",
            "epoch 2 | complete,                     loss = 3.700, val_loss = 3.439\n",
            "total loss: 342.2833364009857\n",
            "24m | epoch 2 |  7% | loss = 3.422833364009857\n",
            "total loss: 337.109050989151\n",
            "25m | epoch 2 |  15% | loss = 3.37109050989151\n",
            "total loss: 330.19935035705566\n",
            "26m | epoch 2 |  23% | loss = 3.3019935035705568\n",
            "total loss: 321.6505980491638\n",
            "27m | epoch 2 |  31% | loss = 3.2165059804916383\n",
            "total loss: 316.12660121917725\n",
            "28m | epoch 2 |  39% | loss = 3.1612660121917724\n",
            "total loss: 310.657372713089\n",
            "29m | epoch 2 |  47% | loss = 3.1065737271308898\n",
            "total loss: 304.5309536457062\n",
            "30m | epoch 2 |  55% | loss = 3.045309536457062\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxXP6gic0RAg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "chencherry = SmoothingFunction()\n",
        "\n",
        "def calculate_bleu_scores(output, gold_output):\n",
        "    for out, gout in zip(output, gold_output):\n",
        "        total.append(nltk.translate.bleu_score.sentence_bleu([gout], out, smoothing_function=chencherry.method0))\n",
        "    print(np.mean(total))\n",
        "\n",
        "\n",
        "def append_to_list(output):\n",
        "    print(output)\n",
        "    for out in output:\n",
        "        line = ' '.join(out)\\\n",
        "                .replace('@@ ', '')\\\n",
        "                .replace('<sos>', '')\\\n",
        "                .replace('<eos>', '')\\\n",
        "                .replace('<unk>', '')\n",
        "        hypothesis.append(line)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OSHLreDxE-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hypothesis = []\n",
        "test_model(model, opt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UcZtOesXKi9b",
        "outputId": "57c82bed-b8cb-42c0-9ebc-bd631cf76471",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "hypothesis[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-a6e8a958e8f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhypothesis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'hypothesis' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXjSNARNeAM4",
        "colab_type": "code",
        "outputId": "eb5c5c0c-577a-4d28-bbc3-1c660c089d9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "pkl_path = '/content/drive/My Drive/Spring-20/11-747/11-747 Project/checkpoints/28-02_23_43-fairseq'\n",
        "\n",
        "with open(pkl_path + '/hypothesis_fq.txt', 'w') as hypothesis_writer:\n",
        "    for x in hypothesis:\n",
        "        hypothesis_writer.write('%s\\n' % x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-1815d08cc78f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpkl_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/My Drive/Spring-20/11-747/11-747 Project/checkpoints/28-02_23_43-fairseq'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkl_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/hypothesis_fq.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhypothesis_writer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhypothesis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mhypothesis_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Spring-20/11-747/11-747 Project/checkpoints/28-02_23_43-fairseq/hypothesis_fq.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFpO9gawtpos",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "goutput = []\n",
        "with open(pkl_path + '/test.en.txt', 'r') as real_output:\n",
        "    for x in real_output:\n",
        "        x = x.strip('\\n')\\\n",
        "                .replace('@@ ', '')\\\n",
        "                .replace('<sos>', '')\\\n",
        "                .replace('<eos>', '')\\\n",
        "                .replace('<unk>', '')\n",
        "        goutput.append(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aUdaGklul-1",
        "colab_type": "code",
        "outputId": "49624933-1bad-4148-e091-ce49ec56045f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "goutput[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['you know , one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways , who still feel their past in the wind , touch it in stones polished by rain , taste it in the bitter leaves of plants .',\n",
              " 'just to know that jaguar shamans still journey beyond the milky way , or the myths of the inuit elders still resonate with meaning , or that in the himalaya , the buddhists still pursue the breath of the dharma , is to really remember the central revelation of anthropology , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the consequence of one particular set of adaptive choices that our lineage made , albeit successfully , many generations ago .',\n",
              " 'and of course , we all share the same adaptive imperatives .',\n",
              " 'we &apos;re all born . we all bring our children into the world .',\n",
              " 'we go through initiation rites .']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEq6maVLunPS",
        "colab_type": "code",
        "outputId": "812239dd-29d9-4976-b30f-ea1bc7756015",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "nltk.translate.bleu_score.corpus_bleu(goutput, hypothesis, smoothing_function=chencherry.method0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6716043329073803"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elGez4ATvLHW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8Q26gvRjzhO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hypothesis = open(\"hypothesis_unsorted.txt\").readlines()\n",
        "hypothesis = [x.strip('\\n').split(' ') for x in hypothesis]\n",
        "\n",
        "reference = open(\"test.txt\").readlines()\n",
        "reference = [[x.strip('\\n').split(' ')] for x in reference]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}