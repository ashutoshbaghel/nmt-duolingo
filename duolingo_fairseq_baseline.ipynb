{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "duolingo-fairseq-baseline.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wkIfRRnwvr2S",
        "x4pC95gJv2Gi",
        "CCn3tOYOxjd7"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0WKxSbOxo0H",
        "colab_type": "code",
        "outputId": "40497f8e-0228-45ae-9b8e-7646e1ea991d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hc1cs31Ep2Gs",
        "colab_type": "code",
        "outputId": "4742a68a-a75e-4161-a2d1-1bd1061e97ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "# ! git clone https://github.com/duolingo/duolingo-sharedtask-2020.git\n",
        "! git clone https://github.com/moses-smt/mosesdecoder\n",
        "! git clone https://github.com/rsennrich/subword-nmt\n",
        "# Install from source\n",
        "! git clone https://github.com/pytorch/fairseq && cd fairseq && pip install --editable ."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'mosesdecoder'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 147544 (delta 12), reused 15 (delta 5), pack-reused 147514\u001b[K\n",
            "Receiving objects: 100% (147544/147544), 129.75 MiB | 22.89 MiB/s, done.\n",
            "Resolving deltas: 100% (113998/113998), done.\n",
            "Cloning into 'subword-nmt'...\n",
            "remote: Enumerating objects: 42, done.\u001b[K\n",
            "remote: Counting objects: 100% (42/42), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 551 (delta 14), reused 23 (delta 7), pack-reused 509\u001b[K\n",
            "Receiving objects: 100% (551/551), 328.51 KiB | 4.76 MiB/s, done.\n",
            "Resolving deltas: 100% (320/320), done.\n",
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 133, done.\u001b[K\n",
            "remote: Counting objects: 100% (133/133), done.\u001b[K\n",
            "remote: Compressing objects: 100% (99/99), done.\u001b[K\n",
            "remote: Total 16235 (delta 63), reused 57 (delta 34), pack-reused 16102\u001b[K\n",
            "Receiving objects: 100% (16235/16235), 7.62 MiB | 9.67 MiB/s, done.\n",
            "Resolving deltas: 100% (11868/11868), done.\n",
            "Obtaining file:///content/fairseq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq==0.9.0) (1.14.0)\n",
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/9d/9846507837ca50ae20917f59d83b79246b8313bd19d4f5bf575ecb98132b/sacrebleu-1.4.9-py3-none-any.whl (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fairseq==0.9.0) (4.38.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fairseq==0.9.0) (0.29.17)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from fairseq==0.9.0) (1.5.0+cu101)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from fairseq==0.9.0) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fairseq==0.9.0) (1.18.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq==0.9.0) (2.20)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu->fairseq==0.9.0) (3.6.6)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/53/84/7b3146ec6378d28abc73ab484f09f47dfa008ad6f03f33d90a369f880e25/portalocker-1.7.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->fairseq==0.9.0) (0.16.0)\n",
            "Installing collected packages: portalocker, sacrebleu, fairseq\n",
            "  Running setup.py develop for fairseq\n",
            "Successfully installed fairseq portalocker-1.7.0 sacrebleu-1.4.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiFGcTQHu_Kv",
        "colab_type": "code",
        "outputId": "1485fd2c-8ffb-4a55-a7db-cf2046ebf6d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "# ! pip install fairseq sacremoses subword_nmt sacrebleu tqdm\n",
        "! pip install sacremoses subword_nmt sacrebleu tqdm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 3.4MB/s \n",
            "\u001b[?25hCollecting subword_nmt\n",
            "  Downloading https://files.pythonhosted.org/packages/74/60/6600a7bc09e7ab38bc53a48a20d8cae49b837f93f5842a41fe513a694912/subword_nmt-0.3.7-py2.py3-none-any.whl\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.6/dist-packages (1.4.9)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.38.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from sacremoses) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses) (0.14.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from sacrebleu) (1.7.0)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu) (3.6.6)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=0fc53ca49e383509e6ca6ebda38699f122bddedfcfa9ff179cef88c2d58f67ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, subword-nmt\n",
            "Successfully installed sacremoses-0.0.43 subword-nmt-0.3.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33GxN25LYzST",
        "colab_type": "code",
        "outputId": "4b5beb62-b38f-48bb-fee5-00f5bcb6ef95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "! rsync -av --exclude=\"checkpoints\" \"/gdrive/My Drive/11-747 Project/duolingo-sharedtask-2020\" ."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sending incremental file list\n",
            "duolingo-sharedtask-2020/\n",
            "duolingo-sharedtask-2020/.gitignore\n",
            "duolingo-sharedtask-2020/README.md\n",
            "duolingo-sharedtask-2020/get_traintest_data.py\n",
            "duolingo-sharedtask-2020/my_cands_extract.py\n",
            "duolingo-sharedtask-2020/preprocess.sh\n",
            "duolingo-sharedtask-2020/run_pretrained.sh\n",
            "duolingo-sharedtask-2020/staple_2020_scorer.py\n",
            "duolingo-sharedtask-2020/train.sh\n",
            "duolingo-sharedtask-2020/utils.py\n",
            "duolingo-sharedtask-2020/variables.sh\n",
            "duolingo-sharedtask-2020/.git/\n",
            "duolingo-sharedtask-2020/.git/HEAD\n",
            "duolingo-sharedtask-2020/.git/config\n",
            "duolingo-sharedtask-2020/.git/description\n",
            "duolingo-sharedtask-2020/.git/index\n",
            "duolingo-sharedtask-2020/.git/packed-refs\n",
            "duolingo-sharedtask-2020/.git/branches/\n",
            "duolingo-sharedtask-2020/.git/hooks/\n",
            "duolingo-sharedtask-2020/.git/hooks/applypatch-msg.sample\n",
            "duolingo-sharedtask-2020/.git/hooks/commit-msg.sample\n",
            "duolingo-sharedtask-2020/.git/hooks/fsmonitor-watchman.sample\n",
            "duolingo-sharedtask-2020/.git/hooks/post-update.sample\n",
            "duolingo-sharedtask-2020/.git/hooks/pre-applypatch.sample\n",
            "duolingo-sharedtask-2020/.git/hooks/pre-commit.sample\n",
            "duolingo-sharedtask-2020/.git/hooks/pre-push.sample\n",
            "duolingo-sharedtask-2020/.git/hooks/pre-rebase.sample\n",
            "duolingo-sharedtask-2020/.git/hooks/pre-receive.sample\n",
            "duolingo-sharedtask-2020/.git/hooks/prepare-commit-msg.sample\n",
            "duolingo-sharedtask-2020/.git/hooks/update.sample\n",
            "duolingo-sharedtask-2020/.git/info/\n",
            "duolingo-sharedtask-2020/.git/info/exclude\n",
            "duolingo-sharedtask-2020/.git/logs/\n",
            "duolingo-sharedtask-2020/.git/logs/HEAD\n",
            "duolingo-sharedtask-2020/.git/logs/refs/\n",
            "duolingo-sharedtask-2020/.git/logs/refs/heads/\n",
            "duolingo-sharedtask-2020/.git/logs/refs/heads/master\n",
            "duolingo-sharedtask-2020/.git/logs/refs/remotes/\n",
            "duolingo-sharedtask-2020/.git/logs/refs/remotes/origin/\n",
            "duolingo-sharedtask-2020/.git/logs/refs/remotes/origin/HEAD\n",
            "duolingo-sharedtask-2020/.git/objects/\n",
            "duolingo-sharedtask-2020/.git/objects/07/\n",
            "duolingo-sharedtask-2020/.git/objects/07/cfe600a0037c870ecf5fdb8b2a5652facfb87a\n",
            "duolingo-sharedtask-2020/.git/objects/09/\n",
            "duolingo-sharedtask-2020/.git/objects/09/438b57db4ea35dc0dade6ade77b7fd2a4834ef\n",
            "duolingo-sharedtask-2020/.git/objects/0a/\n",
            "duolingo-sharedtask-2020/.git/objects/0a/ef9ad113e27023f004acb058db723e751f58ef\n",
            "duolingo-sharedtask-2020/.git/objects/0e/\n",
            "duolingo-sharedtask-2020/.git/objects/0e/8a9d19aba9b607bd6e91974df9bfbd959752aa\n",
            "duolingo-sharedtask-2020/.git/objects/0f/\n",
            "duolingo-sharedtask-2020/.git/objects/0f/a8f2ec0633b17e7ed35a79eae240a2b75a5689\n",
            "duolingo-sharedtask-2020/.git/objects/17/\n",
            "duolingo-sharedtask-2020/.git/objects/17/a41d781cfbef4137078e7e1fadfac874888980\n",
            "duolingo-sharedtask-2020/.git/objects/20/\n",
            "duolingo-sharedtask-2020/.git/objects/20/8fd01b8a97fac0135aa30e7a1d1f057120bfbd\n",
            "duolingo-sharedtask-2020/.git/objects/2a/\n",
            "duolingo-sharedtask-2020/.git/objects/2a/052c9c2f7285583ae4f2dcdde1a56de067d6a0\n",
            "duolingo-sharedtask-2020/.git/objects/38/\n",
            "duolingo-sharedtask-2020/.git/objects/38/14d120ed1368daf84126dd3ece1bdb0bfe6096\n",
            "duolingo-sharedtask-2020/.git/objects/3b/\n",
            "duolingo-sharedtask-2020/.git/objects/3b/3c841bd0817b5cd88d536e25be388bf5fa04f8\n",
            "duolingo-sharedtask-2020/.git/objects/40/\n",
            "duolingo-sharedtask-2020/.git/objects/40/2585a787ac87b0dd5a620829e28b772914285f\n",
            "duolingo-sharedtask-2020/.git/objects/40/8b8b46e33a710cf2b402599c61a3a004500b79\n",
            "duolingo-sharedtask-2020/.git/objects/4a/\n",
            "duolingo-sharedtask-2020/.git/objects/4a/025a6c4d77f333f5d6c5332159fb9e7b75b34f\n",
            "duolingo-sharedtask-2020/.git/objects/4b/\n",
            "duolingo-sharedtask-2020/.git/objects/4b/a98a48dbfb04efdc22d8d36f4091e32f14f316\n",
            "duolingo-sharedtask-2020/.git/objects/4c/\n",
            "duolingo-sharedtask-2020/.git/objects/4c/98f0a8cd51042a767ead1d64d43adf25a14039\n",
            "duolingo-sharedtask-2020/.git/objects/5f/\n",
            "duolingo-sharedtask-2020/.git/objects/5f/81987f0a542e7c9b52f51f7eb2ba648c32cb1c\n",
            "duolingo-sharedtask-2020/.git/objects/60/\n",
            "duolingo-sharedtask-2020/.git/objects/60/52df2fdbc1014bd8f9c581598bd9660cb37583\n",
            "duolingo-sharedtask-2020/.git/objects/62/\n",
            "duolingo-sharedtask-2020/.git/objects/62/6239b78621af96fbb324e678cca17b3dd4e470\n",
            "duolingo-sharedtask-2020/.git/objects/67/\n",
            "duolingo-sharedtask-2020/.git/objects/67/62b8cd7fdaebbbc4289479c0b1355758a5546d\n",
            "duolingo-sharedtask-2020/.git/objects/67/e598b4e10a214bc3cbb1a365c2c8ec432e88d8\n",
            "duolingo-sharedtask-2020/.git/objects/8f/\n",
            "duolingo-sharedtask-2020/.git/objects/8f/b2dfb06b30f5efff2902c2f03678ca97eccf9f\n",
            "duolingo-sharedtask-2020/.git/objects/90/\n",
            "duolingo-sharedtask-2020/.git/objects/90/06610eaad47973d34a11acb28f6e8df56fee5c\n",
            "duolingo-sharedtask-2020/.git/objects/98/\n",
            "duolingo-sharedtask-2020/.git/objects/98/3c720415e6b8912f426f218c28629d05783865\n",
            "duolingo-sharedtask-2020/.git/objects/a0/\n",
            "duolingo-sharedtask-2020/.git/objects/a0/00131a8714a6832069cfc0b281a187e8ed3eb6\n",
            "duolingo-sharedtask-2020/.git/objects/b4/\n",
            "duolingo-sharedtask-2020/.git/objects/b4/f4e11859ce1166118e5e263566c9f25d630c64\n",
            "duolingo-sharedtask-2020/.git/objects/b7/\n",
            "duolingo-sharedtask-2020/.git/objects/b7/752439eaaca23e52d5defee22741d23c2f478a\n",
            "duolingo-sharedtask-2020/.git/objects/c1/\n",
            "duolingo-sharedtask-2020/.git/objects/c1/2b4971dbe72a35f183afbc6c3e097f5dc9d160\n",
            "duolingo-sharedtask-2020/.git/objects/cb/\n",
            "duolingo-sharedtask-2020/.git/objects/cb/612140e661f4dabc4516a6f3e0830490f9d62c\n",
            "duolingo-sharedtask-2020/.git/objects/cc/\n",
            "duolingo-sharedtask-2020/.git/objects/cc/01a9fefede2591e0e1288629de43883d4e8545\n",
            "duolingo-sharedtask-2020/.git/objects/cd/\n",
            "duolingo-sharedtask-2020/.git/objects/cd/35f0c06d83033bc93305e7a1ab32a4a285f4ab\n",
            "duolingo-sharedtask-2020/.git/objects/d2/\n",
            "duolingo-sharedtask-2020/.git/objects/d2/2913982d901ddf9d846d9bab57a405cbc51cbf\n",
            "duolingo-sharedtask-2020/.git/objects/d3/\n",
            "duolingo-sharedtask-2020/.git/objects/d3/2f6ad444125cdef6151591e89a0edd0f6bfcfc\n",
            "duolingo-sharedtask-2020/.git/objects/e4/\n",
            "duolingo-sharedtask-2020/.git/objects/e4/527df8c17d55f9159c9a575ecb915b47281c4c\n",
            "duolingo-sharedtask-2020/.git/objects/ef/\n",
            "duolingo-sharedtask-2020/.git/objects/ef/3d191f481e2cfb52b7202e5525a2ff1e36f675\n",
            "duolingo-sharedtask-2020/.git/objects/f6/\n",
            "duolingo-sharedtask-2020/.git/objects/f6/0466cadc5221b63e830982a35b6773de4fefad\n",
            "duolingo-sharedtask-2020/.git/objects/fd/\n",
            "duolingo-sharedtask-2020/.git/objects/fd/bb5a576636e30a7a2fe3f69e2353d80ee7e2b4\n",
            "duolingo-sharedtask-2020/.git/objects/info/\n",
            "duolingo-sharedtask-2020/.git/objects/pack/\n",
            "duolingo-sharedtask-2020/.git/refs/\n",
            "duolingo-sharedtask-2020/.git/refs/heads/\n",
            "duolingo-sharedtask-2020/.git/refs/heads/master\n",
            "duolingo-sharedtask-2020/.git/refs/remotes/\n",
            "duolingo-sharedtask-2020/.git/refs/remotes/origin/\n",
            "duolingo-sharedtask-2020/.git/refs/remotes/origin/HEAD\n",
            "duolingo-sharedtask-2020/.git/refs/tags/\n",
            "duolingo-sharedtask-2020/.ipynb_checkpoints/\n",
            "duolingo-sharedtask-2020/Drive/\n",
            "duolingo-sharedtask-2020/Drive/Spring-20/\n",
            "duolingo-sharedtask-2020/Drive/Spring-20/11-747/\n",
            "duolingo-sharedtask-2020/Drive/Spring-20/11-747/11-747/\n",
            "duolingo-sharedtask-2020/Project/\n",
            "duolingo-sharedtask-2020/Project/duolingo-sharedtask-2020/\n",
            "duolingo-sharedtask-2020/__pycache__/\n",
            "duolingo-sharedtask-2020/__pycache__/utils.cpython-36.pyc\n",
            "duolingo-sharedtask-2020/data/\n",
            "duolingo-sharedtask-2020/data/courses/\n",
            "duolingo-sharedtask-2020/data/courses/en-hu/\n",
            "duolingo-sharedtask-2020/data/courses/en-hu/bpecode\n",
            "duolingo-sharedtask-2020/data/courses/en-hu/dev-sents.clean.bpe.en\n",
            "duolingo-sharedtask-2020/data/courses/en-hu/dev-sents.clean.bpe.hu\n",
            "duolingo-sharedtask-2020/data/courses/en-hu/dev-sents.en\n",
            "duolingo-sharedtask-2020/data/courses/en-hu/dev-sents.hu\n",
            "duolingo-sharedtask-2020/data/courses/en-hu/test-sents.clean.bpe.en\n",
            "duolingo-sharedtask-2020/data/courses/en-hu/test-sents.clean.bpe.hu\n",
            "duolingo-sharedtask-2020/data/courses/en-hu/test-sents.en\n",
            "duolingo-sharedtask-2020/data/courses/en-hu/test-sents.hu\n",
            "duolingo-sharedtask-2020/data/courses/en-hu/train-sents.clean.bpe.en\n",
            "duolingo-sharedtask-2020/data/courses/en-hu/train-sents.clean.bpe.hu\n",
            "duolingo-sharedtask-2020/data/courses/en-hu/train-sents.en\n",
            "duolingo-sharedtask-2020/data/courses/en-hu/train-sents.hu\n",
            "duolingo-sharedtask-2020/data/courses/en-hu/bin/\n",
            "duolingo-sharedtask-2020/data/courses/en-hu/bin/dict.en.txt\n",
            "duolingo-sharedtask-2020/data/courses/en-hu/bin/dict.hu.txt\n",
            "duolingo-sharedtask-2020/data/courses/en-hu/bin/preprocess.log\n",
            "duolingo-sharedtask-2020/data/courses/en-hu/bin/test.en-hu.en\n",
            "duolingo-sharedtask-2020/data/courses/en-hu/bin/test.en-hu.hu\n",
            "duolingo-sharedtask-2020/data/courses/en-hu/bin/train.en-hu.en\n",
            "duolingo-sharedtask-2020/data/courses/en-hu/bin/train.en-hu.hu\n",
            "duolingo-sharedtask-2020/data/courses/en-hu/bin/valid.en-hu.en\n",
            "duolingo-sharedtask-2020/data/courses/en-hu/bin/valid.en-hu.hu\n",
            "duolingo-sharedtask-2020/data/courses/en-hu/output/\n",
            "duolingo-sharedtask-2020/data/courses/en-hu/output/all_cands.txt\n",
            "duolingo-sharedtask-2020/data/courses/en-hu/output/all_cands_detok.txt\n",
            "duolingo-sharedtask-2020/data/courses/en-hu/output/gen.out\n",
            "duolingo-sharedtask-2020/data/courses/en-hu/output/ref.out\n",
            "duolingo-sharedtask-2020/data/courses/en-hu/output/sys.out\n",
            "duolingo-sharedtask-2020/data/courses/en-ko/\n",
            "duolingo-sharedtask-2020/data/courses/en-ko/bpecode\n",
            "duolingo-sharedtask-2020/data/courses/en-ko/dev-sents.clean.bpe.en\n",
            "duolingo-sharedtask-2020/data/courses/en-ko/dev-sents.clean.bpe.ko\n",
            "duolingo-sharedtask-2020/data/courses/en-ko/dev-sents.en\n",
            "duolingo-sharedtask-2020/data/courses/en-ko/dev-sents.ko\n",
            "duolingo-sharedtask-2020/data/courses/en-ko/test-sents.clean.bpe.en\n",
            "duolingo-sharedtask-2020/data/courses/en-ko/test-sents.clean.bpe.ko\n",
            "duolingo-sharedtask-2020/data/courses/en-ko/test-sents.en\n",
            "duolingo-sharedtask-2020/data/courses/en-ko/test-sents.ko\n",
            "duolingo-sharedtask-2020/data/courses/en-ko/train-sents.clean.bpe.en\n",
            "duolingo-sharedtask-2020/data/courses/en-ko/train-sents.clean.bpe.ko\n",
            "duolingo-sharedtask-2020/data/courses/en-ko/train-sents.en\n",
            "duolingo-sharedtask-2020/data/courses/en-ko/train-sents.ko\n",
            "duolingo-sharedtask-2020/data/courses/en-ko/bin/\n",
            "duolingo-sharedtask-2020/data/courses/en-ko/bin/dict.en.txt\n",
            "duolingo-sharedtask-2020/data/courses/en-ko/bin/dict.ko.txt\n",
            "duolingo-sharedtask-2020/data/courses/en-ko/bin/preprocess.log\n",
            "duolingo-sharedtask-2020/data/courses/en-ko/bin/test.en-ko.en\n",
            "duolingo-sharedtask-2020/data/courses/en-ko/bin/test.en-ko.ko\n",
            "duolingo-sharedtask-2020/data/courses/en-ko/bin/train.en-ko.en\n",
            "duolingo-sharedtask-2020/data/courses/en-ko/bin/train.en-ko.ko\n",
            "duolingo-sharedtask-2020/data/courses/en-ko/bin/valid.en-ko.en\n",
            "duolingo-sharedtask-2020/data/courses/en-ko/bin/valid.en-ko.ko\n",
            "duolingo-sharedtask-2020/data/courses/en-pt/\n",
            "duolingo-sharedtask-2020/data/courses/en-pt/bpecode\n",
            "duolingo-sharedtask-2020/data/courses/en-pt/dev-sents.clean.bpe.en\n",
            "duolingo-sharedtask-2020/data/courses/en-pt/dev-sents.clean.bpe.pt\n",
            "duolingo-sharedtask-2020/data/courses/en-pt/dev-sents.en\n",
            "duolingo-sharedtask-2020/data/courses/en-pt/dev-sents.pt\n",
            "duolingo-sharedtask-2020/data/courses/en-pt/test-sents.clean.bpe.en\n",
            "duolingo-sharedtask-2020/data/courses/en-pt/test-sents.clean.bpe.pt\n",
            "duolingo-sharedtask-2020/data/courses/en-pt/test-sents.en\n",
            "duolingo-sharedtask-2020/data/courses/en-pt/test-sents.pt\n",
            "duolingo-sharedtask-2020/data/courses/en-pt/train-sents.clean.bpe.en\n",
            "duolingo-sharedtask-2020/data/courses/en-pt/train-sents.clean.bpe.pt\n",
            "duolingo-sharedtask-2020/data/courses/en-pt/train-sents.en\n",
            "duolingo-sharedtask-2020/data/courses/en-pt/train-sents.pt\n",
            "duolingo-sharedtask-2020/data/courses/en-pt/bin/\n",
            "duolingo-sharedtask-2020/data/courses/en-pt/bin/dict.en.txt\n",
            "duolingo-sharedtask-2020/data/courses/en-pt/bin/dict.pt.txt\n",
            "duolingo-sharedtask-2020/data/courses/en-pt/bin/preprocess.log\n",
            "duolingo-sharedtask-2020/data/courses/en-pt/bin/test.en-pt.en\n",
            "duolingo-sharedtask-2020/data/courses/en-pt/bin/test.en-pt.pt\n",
            "duolingo-sharedtask-2020/data/courses/en-pt/bin/train.en-pt.en\n",
            "duolingo-sharedtask-2020/data/courses/en-pt/bin/train.en-pt.pt\n",
            "duolingo-sharedtask-2020/data/courses/en-pt/bin/valid.en-pt.en\n",
            "duolingo-sharedtask-2020/data/courses/en-pt/bin/valid.en-pt.pt\n",
            "duolingo-sharedtask-2020/data/courses/en-vi/\n",
            "duolingo-sharedtask-2020/data/courses/en-vi/bpecode\n",
            "duolingo-sharedtask-2020/data/courses/en-vi/dev-sents.clean.bpe.en\n",
            "duolingo-sharedtask-2020/data/courses/en-vi/dev-sents.clean.bpe.vi\n",
            "duolingo-sharedtask-2020/data/courses/en-vi/dev-sents.en\n",
            "duolingo-sharedtask-2020/data/courses/en-vi/dev-sents.vi\n",
            "duolingo-sharedtask-2020/data/courses/en-vi/test-sents.clean.bpe.en\n",
            "duolingo-sharedtask-2020/data/courses/en-vi/test-sents.clean.bpe.vi\n",
            "duolingo-sharedtask-2020/data/courses/en-vi/test-sents.en\n",
            "duolingo-sharedtask-2020/data/courses/en-vi/test-sents.vi\n",
            "duolingo-sharedtask-2020/data/courses/en-vi/train-sents.clean.bpe.en\n",
            "duolingo-sharedtask-2020/data/courses/en-vi/train-sents.clean.bpe.vi\n",
            "duolingo-sharedtask-2020/data/courses/en-vi/train-sents.en\n",
            "duolingo-sharedtask-2020/data/courses/en-vi/train-sents.vi\n",
            "duolingo-sharedtask-2020/data/courses/en-vi/bin/\n",
            "duolingo-sharedtask-2020/data/courses/en-vi/bin/dict.en.txt\n",
            "duolingo-sharedtask-2020/data/courses/en-vi/bin/dict.vi.txt\n",
            "duolingo-sharedtask-2020/data/courses/en-vi/bin/preprocess.log\n",
            "duolingo-sharedtask-2020/data/courses/en-vi/bin/test.en-vi.en\n",
            "duolingo-sharedtask-2020/data/courses/en-vi/bin/test.en-vi.vi\n",
            "duolingo-sharedtask-2020/data/courses/en-vi/bin/train.en-vi.en\n",
            "duolingo-sharedtask-2020/data/courses/en-vi/bin/train.en-vi.vi\n",
            "duolingo-sharedtask-2020/data/courses/en-vi/bin/valid.en-vi.en\n",
            "duolingo-sharedtask-2020/data/courses/en-vi/bin/valid.en-vi.vi\n",
            "duolingo-sharedtask-2020/data/en_hu/\n",
            "duolingo-sharedtask-2020/data/en_hu/dev.en_hu.2020-01-13.gold.txt\n",
            "duolingo-sharedtask-2020/data/en_hu/test.en_hu.2020-01-13.gold.txt\n",
            "duolingo-sharedtask-2020/data/en_hu/train.en_hu.2020-01-13.gold.txt\n",
            "duolingo-sharedtask-2020/data/en_ko/\n",
            "duolingo-sharedtask-2020/data/en_ko/dev.en_ko.2020-01-13.gold.txt\n",
            "duolingo-sharedtask-2020/data/en_ko/test.en_ko.2020-01-13.gold.txt\n",
            "duolingo-sharedtask-2020/data/en_ko/train.en_ko.2020-01-13.gold.txt\n",
            "duolingo-sharedtask-2020/data/en_pt/\n",
            "duolingo-sharedtask-2020/data/en_pt/dev.en_pt.2020-01-13.gold.txt\n",
            "duolingo-sharedtask-2020/data/en_pt/test.en_pt.2020-01-13.gold.txt\n",
            "duolingo-sharedtask-2020/data/en_pt/train.en_pt.2020-01-13.gold.txt\n",
            "duolingo-sharedtask-2020/data/en_vi/\n",
            "duolingo-sharedtask-2020/data/en_vi/dev.en_vi.2020-01-13.gold.txt\n",
            "duolingo-sharedtask-2020/data/en_vi/test.en_vi.2020-01-13.gold.txt\n",
            "duolingo-sharedtask-2020/data/en_vi/train.en_vi.2020-01-13.gold.txt\n",
            "\n",
            "sent 385,469,936 bytes  received 3,686 bytes  4,729,737.69 bytes/sec\n",
            "total size is 385,361,949  speedup is 1.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CR6jK1Rz9L4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! mkdir -p duolingo-sharedtask-2020/checkpoints/fconv/ && cp -r \"/gdrive/My Drive/11-747 Project/duolingo-sharedtask-2020/checkpoints/fconv/\" duolingo-sharedtask-2020/checkpoints/fconv/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXFhOP9IteFe",
        "colab_type": "code",
        "outputId": "2ad43176-5a84-4112-becb-d8ed04fd4ef9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "% cd duolingo-sharedtask-2020/ "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/duolingo-sharedtask-2020\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEdCa667yx_L",
        "colab_type": "text"
      },
      "source": [
        "## Set variables:\n",
        "Set these values in variables.sh file:  \n",
        "\n",
        "MOSES=/content/mosesdecoder  \n",
        "SUBWORDNMT=/content/subword-nmt\n",
        "  \n",
        "Location of the shared task data.  \n",
        "SHARED_TASK_DATA=data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkIfRRnwvr2S",
        "colab_type": "text"
      },
      "source": [
        "## [No need to run again] Generate Train/Val/Test splits (80:10:10)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0gPO-0YwYwK",
        "colab_type": "code",
        "outputId": "4b0d3f48-ebc4-4a9a-8781-91e98c6083a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#upload staple-dataset and then srun this:\n",
        "! tar -xvf /content/staple-2020-train.tar.gz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tar: /content/staple-2020-train.tar.gz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VWoHj5Ow7m6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy.random import choice\n",
        "import os\n",
        "\n",
        "def split_data(staple_folder, src, tgt):\n",
        "    split = \"train\"\n",
        "    with open(f\"{staple_folder}/{src}_{tgt}/train.{src}_{tgt}.2020-01-13.gold.txt\") as f:\n",
        "        lines = f.readlines()\n",
        "    \n",
        "    if not os.path.exists(f\"data/{src}_{tgt}\"):\n",
        "        os.makedirs(f\"data/{src}_{tgt}\")\n",
        "\n",
        "    for line in lines:\n",
        "        with open(f\"data/{src}_{tgt}/{split}.{src}_{tgt}.2020-01-13.gold.txt\", \"a\") as out:\n",
        "            out.write(line)\n",
        "        if len(line.strip()) == 0:\n",
        "            split = choice([\"train\", \"dev\", \"test\"], p= [0.8,0.1,0.1])\n",
        "\n",
        "\n",
        "    print(tgt, \" : Done~\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJRQ6agoExgX",
        "colab_type": "code",
        "outputId": "53da6ca1-f187-4df1-c8ae-f72ed183944e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "# split_data(\"/content/staple-2020-train/en_hu/train.en_hu.2020-01-13.gold.txt\")\n",
        "for tgt in [\"hu\", \"ko\", \"pt\", \"vi\"]:\n",
        "    split_data(\"/content/staple-2020-train\", \"en\", tgt)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-2d467ae8a6bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtgt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"hu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ko\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vi\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msplit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/staple-2020-train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"en\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-130b4ffdbd75>\u001b[0m in \u001b[0;36msplit_data\u001b[0;34m(staple_folder, src, tgt)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstaple_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{staple_folder}/{src}_{tgt}/train.{src}_{tgt}.2020-01-13.gold.txt\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/staple-2020-train/en_hu/train.en_hu.2020-01-13.gold.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4pC95gJv2Gi",
        "colab_type": "text"
      },
      "source": [
        "## Run duolingo/fairseq preprocessing:\n",
        "\n",
        "You'll have to manually change tgt variable in variables.sh file and run for each target.  \n",
        "\n",
        "YOU MAY SEE:  \n",
        "WARNING: No known abbreviations for language 'ja', attempting fall-back to English version.\n",
        "\n",
        "> From forums: That warning, I believe, is coming from the moses tokenizer. It may signify that tokenization is slightly weird which is not usually a problem. However, in the specific case of Japanese, you need a segmenter. For that, we recommend: http://www.phontron.com/kytea/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT8n9yrGwJll",
        "colab_type": "code",
        "outputId": "f3060218-7171-4cde-f25e-a959f88016f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "source": [
        "! bash preprocess.sh"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "dev\n",
            "test\n",
            "pre-processing train data...\n",
            "Tokenizer Version 1.1\n",
            "Language: en\n",
            "Number of threads: 8\n",
            "Tokenizer Version 1.1\n",
            "Language: vi\n",
            "Number of threads: 8\n",
            "WARNING: No known abbreviations for language 'vi', attempting fall-back to English version...\n",
            "Tokenizer Version 1.1\n",
            "Language: en\n",
            "Number of threads: 8\n",
            "Tokenizer Version 1.1\n",
            "Language: vi\n",
            "Number of threads: 8\n",
            "WARNING: No known abbreviations for language 'vi', attempting fall-back to English version...\n",
            "Tokenizer Version 1.1\n",
            "Language: en\n",
            "Number of threads: 8\n",
            "Tokenizer Version 1.1\n",
            "Language: vi\n",
            "Number of threads: 8\n",
            "WARNING: No known abbreviations for language 'vi', attempting fall-back to English version...\n",
            "learning bpe...\n",
            "no pair has frequency >= 2. Stopping\n",
            "Applying BPE to train-sents.clean.en...\n",
            "Applying BPE to train-sents.clean.vi...\n",
            "Applying BPE to dev-sents.clean.en...\n",
            "Applying BPE to dev-sents.clean.vi...\n",
            "Applying BPE to test-sents.clean.en...\n",
            "Applying BPE to test-sents.clean.vi...\n",
            "Files are in data/courses/en-vi/sents.clean.bpe.{en, vi}\n",
            "2020-04-18 00:53:03 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bpe=None, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='raw', destdir='data/courses/en-vi/bin/', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer='nag', padding_factor=8, seed=1, source_lang='en', srcdict=None, target_lang='vi', task='translation', tensorboard_logdir='', testpref='data/courses/en-vi/test-sents.clean.bpe', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, trainpref='data/courses/en-vi/train-sents.clean.bpe', user_dir=None, validpref='data/courses/en-vi/dev-sents.clean.bpe', workers=20)\n",
            "2020-04-18 00:53:19 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data/courses/en-vi/bin/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCn3tOYOxjd7",
        "colab_type": "text"
      },
      "source": [
        "## Backup datasets to GDrive: \"11-747 Project\" Shared Folder\n",
        "\n",
        "^ Start with this folder "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqNjZlmSyLE8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! cp -r /content/duolingo-sharedtask-2020 \"/gdrive/My Drive/11-747 Project\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a44p9uhkRCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir(\"/gdrive/My Drive/Spring-20/11-747/11-747 Project/duolingo-sharedtask-2020\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGMW2lCC1pbG",
        "colab_type": "text"
      },
      "source": [
        "## Start Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tEZlelyJ7uw",
        "colab_type": "code",
        "outputId": "acb2425f-c8e9-4642-a5d7-4798398e083c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "! bash train.sh"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-06 16:58:25 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer', attention_dropout=0.0, best_checkpoint_metric='bleu', bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data/courses/en-ko/bin/', data_buffer_size=0, dataset_impl='raw', ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eval_bleu=True, eval_bleu_args='{\"beam\": 10, \"max_len_a\": 1.2, \"max_len_b\": 10}', eval_bleu_detok='moses', eval_bleu_detok_args=None, eval_bleu_print_samples=True, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=2, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=200, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=8192, max_tokens_valid=8192, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints/fconv/en-ko', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, target_lang=None, task='translation', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001)\n",
            "2020-05-06 16:58:25 | INFO | fairseq.tasks.translation | [en] dictionary: 1376 types\n",
            "2020-05-06 16:58:25 | INFO | fairseq.tasks.translation | [ko] dictionary: 16448 types\n",
            "2020-05-06 16:58:25 | INFO | fairseq.data.data_utils | loaded 253 examples from: data/courses/en-ko/bin/valid.en-ko.en\n",
            "2020-05-06 16:58:25 | INFO | fairseq.data.data_utils | loaded 253 examples from: data/courses/en-ko/bin/valid.en-ko.ko\n",
            "2020-05-06 16:58:25 | INFO | fairseq.tasks.translation | data/courses/en-ko/bin/ valid en-ko 253 examples\n",
            "2020-05-06 16:58:26 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embed_tokens): Embedding(1376, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embed_tokens): Embedding(16448, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (output_projection): Linear(in_features=512, out_features=16448, bias=False)\n",
            "  )\n",
            ")\n",
            "2020-05-06 16:58:26 | INFO | fairseq_cli.train | model transformer, criterion LabelSmoothedCrossEntropyCriterion\n",
            "2020-05-06 16:58:26 | INFO | fairseq_cli.train | num. model params: 53264384 (num. trained: 53264384)\n",
            "2020-05-06 16:58:34 | INFO | fairseq_cli.train | training on 1 GPUs\n",
            "2020-05-06 16:58:34 | INFO | fairseq_cli.train | max tokens per GPU = 8192 and max sentences per GPU = None\n",
            "2020-05-06 16:58:34 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/fconv/en-ko/checkpoint_last.pt\n",
            "2020-05-06 16:58:34 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2020-05-06 16:59:07 | INFO | fairseq.data.data_utils | loaded 566151 examples from: data/courses/en-ko/bin/train.en-ko.en\n",
            "2020-05-06 16:59:38 | INFO | fairseq.data.data_utils | loaded 566151 examples from: data/courses/en-ko/bin/train.en-ko.ko\n",
            "2020-05-06 16:59:38 | INFO | fairseq.tasks.translation | data/courses/en-ko/bin/ train en-ko 566151 examples\n",
            "epoch 001: 100% 494/495 [13:20<00:01,  1.69s/it, loss=8.587, nll_loss=7.785, ppl=220.53, wps=4310, ups=0.62, wpb=6940.3, bsz=1114.9, num_updates=400, lr=5e-05, gnorm=3.157, train_wall=161, wall=715]\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/1 [00:00<?, ?it/s]\u001b[A/pytorch/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n",
            "2020-05-06 17:13:07 | INFO | fairseq.tasks.translation | example hypothesis: 그는그그그그그.\n",
            "2020-05-06 17:13:07 | INFO | fairseq.tasks.translation | example reference: 그는유명한사람이지만나는그를사랑하지않는다.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset: 100% 1/1 [00:03<00:00,  3.20s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset | loss 8.858 | nll_loss 7.937 | ppl 245.1 | bleu 2.93 | wps 0 | wpb 1606 | bsz 253 | num_updates 495\n",
            "epoch 001: 100% 494/495 [13:25<00:01,  1.69s/it, loss=8.587, nll_loss=7.785, ppl=220.53, wps=4310, ups=0.62, wpb=6940.3, bsz=1114.9, num_updates=400, lr=5e-05, gnorm=3.157, train_wall=161, wall=715]2020-05-06 17:13:16 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/en-ko/checkpoint1.pt (epoch 1 @ 495 updates, score 2.93) (writing took 9.437757450000163 seconds)\n",
            "epoch 001 | loss 9.607 | nll_loss 8.947 | ppl 493.62 | wps 4289.6 | ups 0.61 | wpb 7052.8 | bsz 1143.7 | num_updates 495 | lr 6.1875e-05 | gnorm 3.82 | train_wall 800 | wall 882\n",
            "epoch 002: 100% 494/495 [13:19<00:01,  1.68s/it, loss=6.415, nll_loss=5.283, ppl=38.95, wps=4363.8, ups=0.62, wpb=7069.4, bsz=1123.5, num_updates=900, lr=0.0001125, gnorm=2.239, train_wall=162, wall=1538]\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/1 [00:00<?, ?it/s]\u001b[A2020-05-06 17:26:41 | INFO | fairseq.tasks.translation | example hypothesis: 그는그가그가,,,,,, 않습니다.\n",
            "2020-05-06 17:26:41 | INFO | fairseq.tasks.translation | example reference: 그는유명한사람이지만나는그를사랑하지않는다.\n",
            "\n",
            "epoch 002 | valid on 'valid' subset: 100% 1/1 [00:03<00:00,  3.24s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset | loss 8.24 | nll_loss 7.16 | ppl 142.98 | bleu 2.36 | wps 0 | wpb 1606 | bsz 253 | num_updates 990 | best_bleu 2.93\n",
            "epoch 002: 100% 494/495 [13:24<00:01,  1.68s/it, loss=6.415, nll_loss=5.283, ppl=38.95, wps=4363.8, ups=0.62, wpb=7069.4, bsz=1123.5, num_updates=900, lr=0.0001125, gnorm=2.239, train_wall=162, wall=1538]2020-05-06 17:26:45 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/en-ko/checkpoint2.pt (epoch 2 @ 990 updates, score 2.36) (writing took 4.500572416000068 seconds)\n",
            "epoch 002 | loss 6.842 | nll_loss 5.773 | ppl 54.68 | wps 4313.7 | ups 0.61 | wpb 7052.8 | bsz 1143.7 | num_updates 990 | lr 0.00012375 | gnorm 2.322 | train_wall 799 | wall 1691\n",
            "epoch 003: 100% 494/495 [13:18<00:01,  1.59s/it, loss=5.15, nll_loss=3.813, ppl=14.05, wps=4407.4, ups=0.61, wpb=7185.3, bsz=1160.5, num_updates=1400, lr=0.000175, gnorm=1.757, train_wall=163, wall=2356]\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/1 [00:00<?, ?it/s]\u001b[A2020-05-06 17:40:09 | INFO | fairseq.tasks.translation | example hypothesis: 그가그가그가한한위해걸걸걸걸걸걸않습니다.\n",
            "2020-05-06 17:40:09 | INFO | fairseq.tasks.translation | example reference: 그는유명한사람이지만나는그를사랑하지않는다.\n",
            "\n",
            "epoch 003 | valid on 'valid' subset: 100% 1/1 [00:03<00:00,  3.28s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset | loss 7.826 | nll_loss 6.653 | ppl 100.66 | bleu 0 | wps 0 | wpb 1606 | bsz 253 | num_updates 1485 | best_bleu 2.93\n",
            "epoch 003: 100% 494/495 [13:23<00:01,  1.59s/it, loss=5.15, nll_loss=3.813, ppl=14.05, wps=4407.4, ups=0.61, wpb=7185.3, bsz=1160.5, num_updates=1400, lr=0.000175, gnorm=1.757, train_wall=163, wall=2356]2020-05-06 17:40:13 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/en-ko/checkpoint3.pt (epoch 3 @ 1485 updates, score 0.0) (writing took 4.346888983000099 seconds)\n",
            "epoch 003 | loss 5.303 | nll_loss 3.996 | ppl 15.95 | wps 4319.7 | ups 0.61 | wpb 7052.8 | bsz 1143.7 | num_updates 1485 | lr 0.000185625 | gnorm 1.792 | train_wall 798 | wall 2499\n",
            "epoch 004: 100% 494/495 [13:15<00:01,  1.61s/it, loss=4.401, nll_loss=2.92, ppl=7.57, wps=4423.7, ups=0.62, wpb=7085.9, bsz=1121.4, num_updates=1900, lr=0.0002375, gnorm=1.355, train_wall=160, wall=3166]\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/1 [00:00<?, ?it/s]\u001b[A2020-05-06 17:53:33 | INFO | fairseq.tasks.translation | example hypothesis: 그는그가그가그가그가그가그가어떤것도하는아니다.\n",
            "2020-05-06 17:53:33 | INFO | fairseq.tasks.translation | example reference: 그는유명한사람이지만나는그를사랑하지않는다.\n",
            "\n",
            "epoch 004 | valid on 'valid' subset: 100% 1/1 [00:03<00:00,  3.13s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset | loss 7.489 | nll_loss 6.286 | ppl 78.02 | bleu 0 | wps 0 | wpb 1606 | bsz 253 | num_updates 1980 | best_bleu 2.93\n",
            "epoch 004: 100% 494/495 [13:19<00:01,  1.61s/it, loss=4.401, nll_loss=2.92, ppl=7.57, wps=4423.7, ups=0.62, wpb=7085.9, bsz=1121.4, num_updates=1900, lr=0.0002375, gnorm=1.355, train_wall=160, wall=3166]2020-05-06 17:53:38 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/en-ko/checkpoint4.pt (epoch 4 @ 1980 updates, score 0.0) (writing took 4.469067855000503 seconds)\n",
            "epoch 004 | loss 4.528 | nll_loss 3.074 | ppl 8.42 | wps 4339.7 | ups 0.62 | wpb 7052.8 | bsz 1143.7 | num_updates 1980 | lr 0.0002475 | gnorm 1.422 | train_wall 794 | wall 3304\n",
            "epoch 005: 100% 494/495 [13:15<00:01,  1.56s/it, loss=4.1, nll_loss=2.562, ppl=5.91, wps=4416.4, ups=0.62, wpb=7132.4, bsz=1140.6, num_updates=2400, lr=0.0003, gnorm=1.117, train_wall=161, wall=3982]\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/1 [00:00<?, ?it/s]\u001b[A2020-05-06 18:06:58 | INFO | fairseq.tasks.translation | example hypothesis: 그는사랑을, 내가아닙니다.\n",
            "2020-05-06 18:06:58 | INFO | fairseq.tasks.translation | example reference: 그는유명한사람이지만나는그를사랑하지않는다.\n",
            "\n",
            "epoch 005 | valid on 'valid' subset: 100% 1/1 [00:03<00:00,  3.01s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset | loss 7.14 | nll_loss 5.898 | ppl 59.61 | bleu 2.88 | wps 0 | wpb 1606 | bsz 253 | num_updates 2475 | best_bleu 2.93\n",
            "epoch 005: 100% 494/495 [13:20<00:01,  1.56s/it, loss=4.1, nll_loss=2.562, ppl=5.91, wps=4416.4, ups=0.62, wpb=7132.4, bsz=1140.6, num_updates=2400, lr=0.0003, gnorm=1.117, train_wall=161, wall=3982]2020-05-06 18:07:03 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/en-ko/checkpoint5.pt (epoch 5 @ 2475 updates, score 2.88) (writing took 4.510263430999657 seconds)\n",
            "epoch 005 | loss 4.154 | nll_loss 2.627 | ppl 6.18 | wps 4336.2 | ups 0.61 | wpb 7052.8 | bsz 1143.7 | num_updates 2475 | lr 0.000309375 | gnorm 1.149 | train_wall 795 | wall 4109\n",
            "epoch 006: 100% 494/495 [13:15<00:01,  1.61s/it, loss=3.958, nll_loss=2.399, ppl=5.28, wps=4380.6, ups=0.62, wpb=7064.8, bsz=1138.3, num_updates=2900, lr=0.0003625, gnorm=0.986, train_wall=161, wall=4794]\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/1 [00:00<?, ?it/s]\u001b[A2020-05-06 18:20:23 | INFO | fairseq.tasks.translation | example hypothesis: 그는그가그가사랑을하는아니다.\n",
            "2020-05-06 18:20:23 | INFO | fairseq.tasks.translation | example reference: 그는유명한사람이지만나는그를사랑하지않는다.\n",
            "\n",
            "epoch 006 | valid on 'valid' subset: 100% 1/1 [00:02<00:00,  3.00s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset | loss 7.001 | nll_loss 5.751 | ppl 53.86 | bleu 0 | wps 0 | wpb 1606 | bsz 253 | num_updates 2970 | best_bleu 2.93\n",
            "epoch 006: 100% 494/495 [13:19<00:01,  1.61s/it, loss=3.958, nll_loss=2.399, ppl=5.28, wps=4380.6, ups=0.62, wpb=7064.8, bsz=1138.3, num_updates=2900, lr=0.0003625, gnorm=0.986, train_wall=161, wall=4794]2020-05-06 18:20:27 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/en-ko/checkpoint6.pt (epoch 6 @ 2970 updates, score 0.0) (writing took 4.350437363999845 seconds)\n",
            "epoch 006 | loss 3.961 | nll_loss 2.401 | ppl 5.28 | wps 4340 | ups 0.62 | wpb 7052.8 | bsz 1143.7 | num_updates 2970 | lr 0.00037125 | gnorm 1.001 | train_wall 794 | wall 4913\n",
            "epoch 007: 100% 494/495 [13:14<00:01,  1.57s/it, loss=3.879, nll_loss=2.314, ppl=4.97, wps=4412.6, ups=0.62, wpb=7139.4, bsz=1164.2, num_updates=3400, lr=0.000425, gnorm=0.929, train_wall=161, wall=5606]\n",
            "epoch 007 | valid on 'valid' subset:   0% 0/1 [00:00<?, ?it/s]\u001b[A2020-05-06 18:33:46 | INFO | fairseq.tasks.translation | example hypothesis: 그는아니라그는허락하지않습니다.\n",
            "2020-05-06 18:33:46 | INFO | fairseq.tasks.translation | example reference: 그는유명한사람이지만나는그를사랑하지않는다.\n",
            "\n",
            "epoch 007 | valid on 'valid' subset: 100% 1/1 [00:02<00:00,  2.88s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset | loss 7.022 | nll_loss 5.798 | ppl 55.65 | bleu 6.19 | wps 0 | wpb 1606 | bsz 253 | num_updates 3465 | best_bleu 6.19\n",
            "epoch 007: 100% 494/495 [13:18<00:01,  1.57s/it, loss=3.879, nll_loss=2.314, ppl=4.97, wps=4412.6, ups=0.62, wpb=7139.4, bsz=1164.2, num_updates=3400, lr=0.000425, gnorm=0.929, train_wall=161, wall=5606]2020-05-06 18:34:07 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/en-ko/checkpoint7.pt (epoch 7 @ 3465 updates, score 6.19) (writing took 20.813263299000027 seconds)\n",
            "epoch 007 | loss 3.854 | nll_loss 2.281 | ppl 4.86 | wps 4258.4 | ups 0.6 | wpb 7052.8 | bsz 1143.7 | num_updates 3465 | lr 0.000433125 | gnorm 0.889 | train_wall 794 | wall 5733\n",
            "epoch 008: 100% 494/495 [13:14<00:01,  1.60s/it, loss=3.767, nll_loss=2.187, ppl=4.55, wps=4365.8, ups=0.63, wpb=6944.4, bsz=1110.3, num_updates=3900, lr=0.0004875, gnorm=0.899, train_wall=159, wall=6433]\n",
            "epoch 008 | valid on 'valid' subset:   0% 0/1 [00:00<?, ?it/s]\u001b[A2020-05-06 18:47:27 | INFO | fairseq.tasks.translation | example hypothesis: 그는사랑을한것도아니다.\n",
            "2020-05-06 18:47:27 | INFO | fairseq.tasks.translation | example reference: 그는유명한사람이지만나는그를사랑하지않는다.\n",
            "\n",
            "epoch 008 | valid on 'valid' subset: 100% 1/1 [00:02<00:00,  2.96s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset | loss 6.929 | nll_loss 5.69 | ppl 51.62 | bleu 3.93 | wps 0 | wpb 1606 | bsz 253 | num_updates 3960 | best_bleu 6.19\n",
            "epoch 008: 100% 494/495 [13:19<00:01,  1.60s/it, loss=3.767, nll_loss=2.187, ppl=4.55, wps=4365.8, ups=0.63, wpb=6944.4, bsz=1110.3, num_updates=3900, lr=0.0004875, gnorm=0.899, train_wall=159, wall=6433]2020-05-06 18:47:31 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/en-ko/checkpoint8.pt (epoch 8 @ 3960 updates, score 3.93) (writing took 4.567640797999957 seconds)\n",
            "epoch 008 | loss 3.782 | nll_loss 2.204 | ppl 4.61 | wps 4341.5 | ups 0.62 | wpb 7052.8 | bsz 1143.7 | num_updates 3960 | lr 0.000495 | gnorm 0.931 | train_wall 794 | wall 6537\n",
            "epoch 009: 100% 494/495 [13:14<00:01,  1.68s/it, loss=3.735, nll_loss=2.162, ppl=4.48, wps=4367, ups=0.62, wpb=7051.5, bsz=1147.8, num_updates=4400, lr=0.000476731, gnorm=0.793, train_wall=161, wall=7245]\n",
            "epoch 009 | valid on 'valid' subset:   0% 0/1 [00:00<?, ?it/s]\u001b[A2020-05-06 19:00:50 | INFO | fairseq.tasks.translation | example hypothesis: 그는남자고그녀는사람이야.\n",
            "2020-05-06 19:00:50 | INFO | fairseq.tasks.translation | example reference: 그는유명한사람이지만나는그를사랑하지않는다.\n",
            "\n",
            "epoch 009 | valid on 'valid' subset: 100% 1/1 [00:02<00:00,  2.95s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset | loss 7.094 | nll_loss 5.899 | ppl 59.66 | bleu 2.2 | wps 0 | wpb 1606 | bsz 253 | num_updates 4455 | best_bleu 6.19\n",
            "epoch 009: 100% 494/495 [13:18<00:01,  1.68s/it, loss=3.735, nll_loss=2.162, ppl=4.48, wps=4367, ups=0.62, wpb=7051.5, bsz=1147.8, num_updates=4400, lr=0.000476731, gnorm=0.793, train_wall=161, wall=7245]2020-05-06 19:00:55 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/en-ko/checkpoint9.pt (epoch 9 @ 4455 updates, score 2.2) (writing took 4.484592471000724 seconds)\n",
            "epoch 009 | loss 3.716 | nll_loss 2.139 | ppl 4.4 | wps 4345 | ups 0.62 | wpb 7052.8 | bsz 1143.7 | num_updates 4455 | lr 0.000473779 | gnorm 0.819 | train_wall 793 | wall 7341\n",
            "epoch 010: 100% 494/495 [13:15<00:01,  1.56s/it, loss=3.651, nll_loss=2.076, ppl=4.22, wps=4381.7, ups=0.62, wpb=7042.5, bsz=1127.8, num_updates=4900, lr=0.000451754, gnorm=0.908, train_wall=160, wall=8057]\n",
            "epoch 010 | valid on 'valid' subset:   0% 0/1 [00:00<?, ?it/s]\u001b[A2020-05-06 19:14:15 | INFO | fairseq.tasks.translation | example hypothesis: 그는아니라아니야.\n",
            "2020-05-06 19:14:15 | INFO | fairseq.tasks.translation | example reference: 그는유명한사람이지만나는그를사랑하지않는다.\n",
            "\n",
            "epoch 010 | valid on 'valid' subset: 100% 1/1 [00:02<00:00,  2.92s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset | loss 6.876 | nll_loss 5.65 | ppl 50.21 | bleu 7.91 | wps 0 | wpb 1606 | bsz 253 | num_updates 4950 | best_bleu 7.91\n",
            "epoch 010: 100% 494/495 [13:19<00:01,  1.56s/it, loss=3.651, nll_loss=2.076, ppl=4.22, wps=4381.7, ups=0.62, wpb=7042.5, bsz=1127.8, num_updates=4900, lr=0.000451754, gnorm=0.908, train_wall=160, wall=8057]2020-05-06 19:14:28 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/en-ko/checkpoint10.pt (epoch 10 @ 4950 updates, score 7.91) (writing took 13.637432420999176 seconds)\n",
            "epoch 010 | loss 3.642 | nll_loss 2.061 | ppl 4.17 | wps 4291.1 | ups 0.61 | wpb 7052.8 | bsz 1143.7 | num_updates 4950 | lr 0.000449467 | gnorm 0.783 | train_wall 794 | wall 8154\n",
            "epoch 011: 100% 494/495 [13:13<00:01,  1.62s/it, loss=3.56, nll_loss=1.976, ppl=3.93, wps=4411.1, ups=0.62, wpb=7103.6, bsz=1163.6, num_updates=5400, lr=0.000430331, gnorm=0.649, train_wall=161, wall=8877]\n",
            "epoch 011 | valid on 'valid' subset:   0% 0/1 [00:00<?, ?it/s]\u001b[A2020-05-06 19:27:46 | INFO | fairseq.tasks.translation | example hypothesis: 그는한것도아니라아니야.\n",
            "2020-05-06 19:27:46 | INFO | fairseq.tasks.translation | example reference: 그는유명한사람이지만나는그를사랑하지않는다.\n",
            "\n",
            "epoch 011 | valid on 'valid' subset: 100% 1/1 [00:02<00:00,  2.92s/it]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset | loss 6.802 | nll_loss 5.583 | ppl 47.95 | bleu 0 | wps 0 | wpb 1606 | bsz 253 | num_updates 5445 | best_bleu 7.91\n",
            "epoch 011: 100% 494/495 [13:17<00:01,  1.62s/it, loss=3.56, nll_loss=1.976, ppl=3.93, wps=4411.1, ups=0.62, wpb=7103.6, bsz=1163.6, num_updates=5400, lr=0.000430331, gnorm=0.649, train_wall=161, wall=8877]2020-05-06 19:27:51 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/en-ko/checkpoint11.pt (epoch 11 @ 5445 updates, score 0.0) (writing took 4.670415798000249 seconds)\n",
            "epoch 011 | loss 3.584 | nll_loss 2.001 | ppl 4 | wps 4349 | ups 0.62 | wpb 7052.8 | bsz 1143.7 | num_updates 5445 | lr 0.00042855 | gnorm 0.794 | train_wall 793 | wall 8957\n",
            "epoch 012: 100% 494/495 [13:15<00:01,  1.57s/it, loss=3.513, nll_loss=1.93, ppl=3.81, wps=4416.8, ups=0.62, wpb=7128.4, bsz=1149.6, num_updates=5900, lr=0.000411693, gnorm=0.891, train_wall=161, wall=9690]\n",
            "epoch 012 | valid on 'valid' subset:   0% 0/1 [00:00<?, ?it/s]\u001b[A2020-05-06 19:41:11 | INFO | fairseq.tasks.translation | example hypothesis: 그는과일한적없습니다.\n",
            "2020-05-06 19:41:11 | INFO | fairseq.tasks.translation | example reference: 그는유명한사람이지만나는그를사랑하지않는다.\n",
            "\n",
            "epoch 012 | valid on 'valid' subset: 100% 1/1 [00:02<00:00,  2.94s/it]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset | loss 6.783 | nll_loss 5.565 | ppl 47.35 | bleu 0 | wps 0 | wpb 1606 | bsz 253 | num_updates 5940 | best_bleu 7.91\n",
            "epoch 012: 100% 494/495 [13:19<00:01,  1.57s/it, loss=3.513, nll_loss=1.93, ppl=3.81, wps=4416.8, ups=0.62, wpb=7128.4, bsz=1149.6, num_updates=5900, lr=0.000411693, gnorm=0.891, train_wall=161, wall=9690]2020-05-06 19:41:16 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/en-ko/checkpoint12.pt (epoch 12 @ 5940 updates, score 0.0) (writing took 4.449304519999714 seconds)\n",
            "epoch 012 | loss 3.537 | nll_loss 1.957 | ppl 3.88 | wps 4340 | ups 0.62 | wpb 7052.8 | bsz 1143.7 | num_updates 5940 | lr 0.000410305 | gnorm 0.716 | train_wall 794 | wall 9762\n",
            "epoch 013:  87% 432/495 [11:34<01:44,  1.66s/it, loss=3.519, nll_loss=1.944, ppl=3.85, wps=4388.9, ups=0.62, wpb=7059.2, bsz=1164.3, num_updates=6300, lr=0.00039841, gnorm=0.658, train_wall=160, wall=10340]Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/fairseq-train\", line 11, in <module>\n",
            "    load_entry_point('fairseq', 'console_scripts', 'fairseq-train')()\n",
            "  File \"/content/fairseq/fairseq_cli/train.py\", line 359, in cli_main\n",
            "    main(args)\n",
            "  File \"/content/fairseq/fairseq_cli/train.py\", line 117, in main\n",
            "    valid_losses = train(args, trainer, task, epoch_itr, max_update)\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 52, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/content/fairseq/fairseq_cli/train.py\", line 187, in train\n",
            "    log_output = trainer.train_step(samples)\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 52, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/content/fairseq/fairseq/trainer.py\", line 377, in train_step\n",
            "    ignore_grad=is_dummy_batch,\n",
            "  File \"/content/fairseq/fairseq/tasks/fairseq_task.py\", line 343, in train_step\n",
            "    optimizer.backward(loss)\n",
            "  File \"/content/fairseq/fairseq/optim/fairseq_optimizer.py\", line 81, in backward\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/tensor.py\", line 198, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\", line 100, in backward\n",
            "    allow_unreachable=True)  # allow_unreachable flag\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGSvSkD36MEx",
        "colab_type": "code",
        "outputId": "4181aa7c-3abe-425e-9e40-9bf6cba18f4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "! bash run_pretrained.sh checkpoints/fconv/fconv/en-vi/checkpoint_best.pt\n",
        "# ! ls checkpoints/"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/duolingo-sharedtask-2020\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/fairseq-generate\", line 11, in <module>\n",
            "    load_entry_point('fairseq', 'console_scripts', 'fairseq-generate')()\n",
            "  File \"/content/fairseq/fairseq_cli/generate.py\", line 269, in cli_main\n",
            "    main(args)\n",
            "  File \"/content/fairseq/fairseq_cli/generate.py\", line 36, in main\n",
            "    return _main(args, sys.stdout)\n",
            "  File \"/content/fairseq/fairseq_cli/generate.py\", line 72, in _main\n",
            "    task=task,\n",
            "  File \"/content/fairseq/fairseq/checkpoint_utils.py\", line 190, in load_model_ensemble\n",
            "    filenames, arg_overrides, task, strict, suffix,\n",
            "  File \"/content/fairseq/fairseq/checkpoint_utils.py\", line 211, in load_model_ensemble_and_task\n",
            "    model.load_state_dict(state[\"model\"], strict=strict, args=args)\n",
            "  File \"/content/fairseq/fairseq/models/fairseq_model.py\", line 93, in load_state_dict\n",
            "    return super().load_state_dict(new_state_dict, strict)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 847, in load_state_dict\n",
            "    self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n",
            "RuntimeError: Error(s) in loading state_dict for TransformerModel:\n",
            "\tsize mismatch for encoder.embed_tokens.weight: copying a param with shape torch.Size([1648, 512]) from checkpoint, the shape in current model is torch.Size([2224, 512]).\n",
            "\tsize mismatch for decoder.embed_tokens.weight: copying a param with shape torch.Size([1800, 512]) from checkpoint, the shape in current model is torch.Size([8984, 512]).\n",
            "\tsize mismatch for decoder.output_projection.weight: copying a param with shape torch.Size([1800, 512]) from checkpoint, the shape in current model is torch.Size([8984, 512]).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lc62cEPM6weB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! cp -r data/courses/en-pt/output/ \"/gdrive/My Drive/11-747 Project/duolingo-sharedtask-2020/data/courses/en-pt/\" "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TA5fD4o4j_gz",
        "colab_type": "code",
        "outputId": "1a303c93-5476-47ed-aee8-54aa9a5fe0d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "! ls checkpoints/fconv/fconv/en-vi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint30.pt  checkpoint31.pt  checkpoint_best.pt  checkpoint_last.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEXfAXm5kSZi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}